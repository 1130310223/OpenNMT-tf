# The directory where models and summaries will be saved.
# It is created if it does not exist.
model_dir: "ende"

data:
  train_features_file: "data/ende/src-train.txt"
  train_labels_file: "data/ende/tgt-train.txt"
  eval_features_file: "data/ende/src-val.txt.26"
  eval_labels_file: "data/ende/tgt-val.txt.26"

  # (optional) Models may require additional resource files (e.g. vocabularies).
  source_words_vocabulary: "data/ende/src-vocab.txt"
  target_words_vocabulary: "data/ende/tgt-vocab.txt"

params:
  # The optimizer as a string (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/optimizers.py#L40).
  optimizer: SGD
  learning_rate: 1.0

  # (optional) Maximum gradients norm (default: None).
  clip_gradients: 5.0
  # (optional) The type of learning rate decay (default: None). See:
  #  * https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate
  #  * opennmt/utils/decay.py
  # This value may change the semantics of other decay options. See the documentation or the code.
  decay_type: "exponential_decay"
  # (optional unless decay_type is set) The learning rate decay rate.
  decay_rate: 0.7
  # (optional unless decay_type is set) Decay every this many steps.
  decay_steps: 10000
  # (optional) If true, the learning rate is decayed in a staircase fashion (default: True).
  staircase: true
  # (optional) After how many steps to start the decay (default: 0).
  start_decay_steps: 50000
  # (optional) Stop decay when this learning rate value is reached (default: 0).
  minimum_learning_rate: 0.0001
  # (optional) Probability to sample of sampling categorically from the output ids
  # instead of reading directly from the inputs (default: 0).
  scheduled_sampling_probability: 0.1
  # (optional) Width of the beam search (default: 1).
  beam_width: 5
  # (optional) Length penaly weight to apply on hypotheses (default: 0).
  length_penalty: 0.2
  # (optional) Maximum decoding iterations before stopping (default: 250).
  maximum_iterations: 10

train:
  batch_size: 64

  # (optional) Save a checkpoint every this many steps.
  save_checkpoints_steps: 2
  # (optional) How many checkpoints to keep on disk.
  keep_checkpoint_max: 3
  # (optional) Save summaries every this many steps.
  save_summary_steps: 100
  # (optional) Train for this many steps. If not set, train forever.
  train_steps: 1000000
  # (optional) Evaluate every this many steps.
  eval_steps: 5
  # (optional) The maximum length of feature sequences during training (default: None).
  maximum_features_length: 70
  # (optional) The maximum length of label sequences during training (default: None).
  maximum_labels_length: 70
  # (optional) The number of buckets by sequence length to improve training efficiency (default: 5).
  num_buckets: 5
  # (optional) The number of threads to use for processing data in parallel (default: 4).
  num_threads: 4
  # (optional) The data pre-fetch buffer size, e.g. for shuffling examples (default: 10000).
  buffer_size: 10000

infer:
  batch_size: 10

  # (optional) The number of threads to use for processing data in parallel (default: 4).
  num_threads: 4
  # (optional) The data pre-fetch buffer size when processing data in parallel (default: 100).
  buffer_size: 100
  # (optional) For compatible models, the number of hypotheses to output (default: 1).
  n_best: 1

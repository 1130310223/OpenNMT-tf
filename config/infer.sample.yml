# Inference configuration template.

run:
  type: infer

  # The last checkpoint from this directory will be loaded.
  model_dir: "enfr"

data:
  features_file: "data/en-test.txt"

  meta:
    source_words_vocabulary: "data/en-dict.txt"
    target_words_vocabulary: "data/fr-dict.txt"

params:
  batch_size: 1

  # (optional) Width of the beam search.
  beam_width: 5

  # (optional) Length penaly weight to apply on hypotheses.
  length_penalty: 0.2

  # (optional) Maximum decoding iterations before stopping.
  maximum_iterations: 250

  # (optional) How many hypotheses to predict.
  n_best: 1

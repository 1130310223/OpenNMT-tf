# Training configuration template.
#
# The command line accepts multiple configuration files. You can split
# this template to make some configurations (e.g. the optimization) reusable.

# Run description.
run:
  # The type of the run, either "train" or "infer".
  type: train

  # The directory where models and summaries will be saved.
  # It is created if it does not exist.
  model_dir: "ckpt"

  # (optional) Save a checkpoint every this many steps.
  save_checkpoints_steps: 5000

  # (optional) How many checkpoints to keep on disk.
  keep_checkpoint_max: 5

  # (optional) Save summaries every this many steps.
  save_summary_steps: 100

  # (optional) Evaluate every this many steps.
  eval_steps: 10000

  # (optional) If true, GPU memory will be allocated dynamically, otherwise
  # all the memory will be mapped by TensorFlow.
  gpu_allow_growth: true

# (optional) Hosts for distributed training.
hosts:
  # List of master hosts for distributed training (usually 1).
  # The master manages checkpoints, summaries, and evaluation.
  masters:
    - "localhost:2224"

  # List of worker hosts.
  workers:
    - "localhost:2223"

  # List of parameter server hosts.
  ps:
    - "localhost:2222"

# Paths to data files.
data:
  train_features_file: "data/en.txt"
  train_labels_file: "data/fr.txt"
  eval_features_file: "data/en-test.txt"
  eval_labels_file: "data/fr-test.txt"

  # (optional) The maximum length of feature sequences during training (if it applies).
  maximum_features_length: 70

  # (optional) The maximum length of label sequences during training (if it applies).
  maximum_labels_length: 70

  # (optional) The pre-fetch buffer size (e.g. for shuffling examples).
  buffer_size: 10000

  # (optional) The number of buckets by sequence length.
  num_buckets: 5

# Dynamic hyperparameters. Values are given for example and are likely not
# the best configuration for your training.
params:
  batch_size: 64

  # The optimizer as a string (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/optimizers.py#L40).
  optimizer: SGD
  learning_rate: 1.0

  # (optional) Maximum gradients norm.
  clip_gradients: 5.0

  # (optional) The type of learning rate decay (see https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate).
  decay_type: "exponential_decay"

  # (optional unless decay_type is set) The learning rate decay rate.
  decay_rate: 0.7

  # (optional unless decay_type is set) Decay every this many steps.
  decay_steps: 10000

  # (optional) If true, the learning rate is decayed in a staircase fashion (the default).
  staircase: true

  # # (optional) After how many steps to start the decay.
  start_decay_steps: 50000

  # (optional) Stop decay when this learning rate value is reached.
  minimum_learning_rate: 0.0001

# Training configuration template.

# Run description.
run:
  # The type of the run, either "train" or "infer".
  type: train

  # The directory where models and summaries will be saved.
  # It is created if it does not exist.
  model_dir: "enfr"

  # (optional) Save a checkpoint every this many steps.
  save_checkpoints_steps: 5000

  # (optional) How many checkpoints to keep on disk.
  keep_checkpoint_max: 3

  # (optional) Save summaries every this many steps.
  save_summary_steps: 100

  # (optional) Train for this many steps. If not set, train forever.
  train_steps: 1000000

  # (optional) Evaluate every this many steps.
  eval_steps: 10000

  # (optional) If true, GPU memory will be allocated dynamically, otherwise
  # all the memory will be mapped by TensorFlow.
  gpu_allow_growth: true

# Paths to data files.
data:
  train_features_file: "data/en.txt"
  train_labels_file: "data/fr.txt"
  eval_features_file: "data/en-test.txt"
  eval_labels_file: "data/fr-test.txt"

  # Models may require additional resource files (e.g. vocabularies).
  meta:
    source_words_vocabulary: "data/en-dict.txt"
    target_words_vocabulary: "data/fr-dict.txt"

  # (optional) The maximum length of feature sequences during training (if it applies).
  maximum_features_length: 70

  # (optional) The maximum length of label sequences during training (if it applies).
  maximum_labels_length: 70

  # (optional) The pre-fetch buffer size (e.g. for shuffling examples).
  buffer_size: 10000

  # (optional) The number of buckets by sequence length.
  num_buckets: 5

# Dynamic hyperparameters. Values are given for example and are likely not
# the best configuration for your training.
params:
  batch_size: 64

  # (optional) Probability to sample of sampling categorically from the output ids
  # instead of reading directly from the inputs.
  scheduled_sampling_probability: 0.1

  # The optimizer as a string (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/optimizers.py#L40).
  optimizer: SGD
  learning_rate: 1.0

  # (optional) Maximum gradients norm.
  clip_gradients: 5.0

  # (optional) The type of learning rate decay. See:
  #  * https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate
  #  * opennmt/utils/decay.py
  # This value may change the semantics of other decay options. See the documentation or the code.
  decay_type: "exponential_decay"

  # (optional unless decay_type is set) The learning rate decay rate.
  decay_rate: 0.7

  # (optional unless decay_type is set) Decay every this many steps.
  decay_steps: 10000

  # (optional) If true, the learning rate is decayed in a staircase fashion (the default).
  staircase: true

  # # (optional) After how many steps to start the decay.
  start_decay_steps: 50000

  # (optional) Stop decay when this learning rate value is reached.
  minimum_learning_rate: 0.0001

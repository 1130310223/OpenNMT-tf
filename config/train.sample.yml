# Run description.
run:
  # The type of the run, either "train" or "infer".
  type: train

  # The directory where models and summaries will be saved.
  # It is created if it does not exist.
  model_dir: "ckpt"

  # (optional) Save a checkpoint every this many steps.
  save_checkpoints_steps: 1000

  # (optional) How many checkpoints to keep on disk.
  keep_checkpoint_max: 3

  # (optional) Save summaries every this many steps.
  save_summary_steps: 50

  # (optional) Evaluate every this many steps.
  eval_steps: 5000

  # (optional) If true, GPU memory will be allocated dynamically, otherwise
  # all the memory will be mapped by TensorFlow.
  gpu_allow_growth: true

# (optional) Hosts for distributed training.
hosts:
  # List of master hosts for distributed training (usually 1).
  # The master manages checkpoints, summaries, and evaluation.
  masters:
    - "localhost:2224"

  # List of worker hosts.
  workers:
    - "localhost:2223"

  # List of parameter server hosts.
  ps:
    - "localhost:2222"

# Paths to data files.
data:
  train_features_file: "data/en.txt"
  train_labels_file: "data/fr.txt"
  eval_features_file: "data/en-test.txt"
  eval_labels_file: "data/fr-test.txt"

  # (optional) The pre-fetch buffer size (e.g. for shuffling examples).
  buffer_size: 10000

  # (optional) The number of buckets by sequence length.
  num_buckets: 5

# Dynamic hyperparameters.
params:
  batch_size: 64

  # The optimizer as a string (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/optimizers.py#L40)
  optimizer: Adam
  learning_rate: 0.0002

  # (optional) Maximum gradients norm.
  clip_gradients: 5.0
